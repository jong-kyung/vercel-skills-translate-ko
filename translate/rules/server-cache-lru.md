---
title: 요청 간 LRU 캐싱
impact: HIGH
impactDescription: 요청 간 캐시
tags: server, cache, lru, cross-request
---

## 요청 간 LRU 캐싱

`React.cache()`는 하나의 요청 안에서만 동작한다. 연속 요청 간 공유되는 데이터(사용자가 A 버튼을 누르고 B 버튼을 누르는 경우 등)에는 LRU 캐시를 사용한다.

**구현:**

```typescript
import { LRUCache } from 'lru-cache'

const cache = new LRUCache<string, any>({
  max: 1000,
  ttl: 5 * 60 * 1000  // 5 minutes
})

export async function getUser(id: string) {
  const cached = cache.get(id)
  if (cached) return cached

  const user = await db.user.findUnique({ where: { id } })
  cache.set(id, user)
  return user
}

// Request 1: DB query, result cached
// Request 2: cache hit, no DB query
```

사용자가 연속적으로 여러 엔드포인트를 호출하면서 같은 데이터를 필요로 할 때 유용하다.

**Vercel의 [Fluid Compute](https://vercel.com/docs/fluid-compute)에서는:** 여러 동시 요청이 동일한 함수 인스턴스와 캐시를 공유할 수 있어 LRU 캐싱이 특히 효과적이다. 외부 저장소(예: Redis) 없이도 요청 간 캐시가 유지된다.

**전통적 서버리스에서는:** 각 호출이 격리되어 있으므로 요청 간 캐싱이 필요하다면 Redis를 고려한다.

참고: [https://github.com/isaacs/node-lru-cache](https://github.com/isaacs/node-lru-cache)
